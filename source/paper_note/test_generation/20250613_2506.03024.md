# 250613_GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models

---
**论文信息**

- **标题**: GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models
- **arXiv ID**: 2506.03024
- **作者**: Authors:Madhusudan Srinivasan, Jubril Abdel
- **发表日期**: 2025-06-03T16:00:30+00:00
- **论文链接**: [2506.03024](https://arxiv.org/abs/2506.03024)
- **总结生成时间**: 2025-06-13 15:03:02

---

**一句话概要**  
GenFair通过等价划分、变异算子和边界值分析的系统化测试生成方法，显著提升了大型语言模型（LLMs）中交叉性公平性缺陷的检测能力。

**主体**  
随着大型语言模型在关键领域的广泛应用，其训练数据中隐含的偏见可能导致公平性风险，尤其是涉及多重身份交叉（如性别与种族叠加）的复杂偏见场景。现有基于模板或语法规则的测试方法（如CheckList和ASTRAEA）存在测试多样性不足、对交叉性偏见的敏感度有限等问题。作者提出GenFair框架，核心创新在于将软件工程中的等价划分、变异测试和边界值分析引入公平性测试领域，通过系统化生成源测试用例并利用蜕变关系（MR）衍生后续用例，再对比模型对源用例与后续用例的响应语气差异来识别公平性违规。

实验环节在GPT-4.0和LLaMA-3.0上验证了GenFair的优越性。其故障检测率（FDR）分别达到0.73和0.69，显著超过模板方法（0.54/0.51）和ASTRAEA（0.39/0.36）。测试用例的多样性指标（句法多样性10.06，语义多样性76.68）和连贯性评分（句法连贯性291.32，语义连贯性0.7043）均优于基线方法，表明生成的测试用例既能覆盖复杂偏见场景，又保持了自然语言的真实性。可视化分析进一步显示，GenFair能有效捕捉传统方法遗漏的交叉性偏见模式。

**最后一句**  
该研究为自动化公平性测试提供了可扩展的解决方案，其方法论启示可延伸至其他AI系统的伦理评估领域。