# 250613_ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests

---
**论文信息**

- **标题**: ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests
- **arXiv ID**: 2506.04894
- **作者**: Authors:Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen
- **发表日期**: 2025-06-05T11:20:37+00:00
- **论文链接**: [2506.04894](https://arxiv.org/abs/2506.04894)
- **总结生成时间**: 2025-06-13 15:03:02

---

**一句话概要**  
作者提出ICPC-Eval基准测试，通过国际大学生程序设计竞赛（ICPC）题目评估大语言模型在真实编程竞赛环境中的复杂推理能力，并设计迭代修复指标Refine@K以捕捉模型的动态优化潜力。

**主体**  
当前大语言模型在代码生成任务上虽取得显著进展，但现有评估体系存在两大局限：传统基准如LiveCodeBench难以模拟真实竞赛环境的高压场景，而Pass@K等指标无法衡量模型通过反馈迭代优化代码的反思能力。为此，研究团队从全球11场ICPC赛事中精选118道题目构建ICPC-Eval，其核心价值在于三重创新：首先，题目类型与难度分布严格对标实际竞赛，包含时间约束和复杂逻辑的综合性挑战；其次，开发自动化测试用例生成工具实现高效本地评估，通过多维度测试覆盖边界条件；最后提出Refine@K指标，允许模型基于执行错误进行多轮代码修正，从而量化其持续改进能力。

实验结果表明，即使顶尖模型如DeepSeek-R1也高度依赖多轮反馈才能充分释放推理潜力——在初始尝试中仅能解决约40%的中等难度题目，但经过5次迭代后正确率提升至67%。横向对比显示，模型群体与人类金牌团队仍存在约30%的性能差距，尤其在需要创造性算法设计的题目上表现明显逊色。值得注意的是，测试案例揭示模型常陷入局部优化陷阱，例如反复修正同一类语法错误却忽视深层逻辑缺陷。

**启示**  
这项研究不仅为衡量AI系统的动态推理能力提供了方法论突破，更揭示了当前大语言模型在复杂问题求解中"重生成轻验证"的固有缺陷，为未来研发具有人类式调试能力的AI系统指明了方向。