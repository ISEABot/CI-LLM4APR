# =============================
# LLM4ArxivPaper Main Configuration Template
#
# Notes:
# 1. All fields can be adjusted as needed; delete unused array configurations if not required.
# 2. If a field supports reading from environment variables, ${VAR_NAME} format is used in examples.
# 3. This template aims to help quickly understand meanings; please streamline as needed before actual run.
# =============================

openai:
  # âœ… OpenAI API Key, supports injection via OPENAI_API_KEY environment variable
  api_key: "${API_KEY}"

  # âœ… If using Azure/OpenAI compatible service, specify `base_url`; leave empty for default official address
  base_url: "${BASE_URL}"

  # âœ… Model for relevance scoring (recommended to use lightweight model, e.g. gpt-4o-mini)
  relevance_model: "gpt-4o-mini"

  # âœ… Model for TODO planning and item-by-item Q&A (can be same as above or use stronger model)
  summarization_model: "gpt-4o"

  # âœ… Global temperature, controls generation randomness (0~1, lower = more stable)
  temperature: 0.2

  # âœ… Output language: 'zh-CN' (Simplified Chinese) or 'en' (English)
  language: "zh-CN"

fetch:
  # âœ… Max papers to fetch per Topic, used to control costs
  max_papers_per_topic: 30

  # âœ… Query lookback days: e.g., 7 means fetch papers published within last 7 days
  days_back: 7

  # âœ… arXiv API request interval (seconds), avoid rate limiting
  request_delay: 3.0

topics:

  # === â… . LLMs for Code ===
  - name: "LLMsForCode"
    label: "LLMs for Code"
    query:
      categories: ["cs.SE", "cs.AI", "cs.CL", "cs.LG"]
      include: [
        "large language model",
        "code language model",
        "code representation",
        "code completion",
        "code understanding",
        "code translation",
        "code summarization"
      ]
      exclude: ["quantum", "biomedical", "vision"]
    interest_prompt: |
      I am interested in the foundation of large language models for code,
      including their architectures (encoder-only, encoder-decoder, decoder-only),
      pretraining objectives, and downstream code tasks such as completion,
      translation, summarization, and defect detection.

  # === â…¡. SE with LLMs: Requirement & Design ===
  - name: "RequirementDesign"
    label: "Requirement and Design"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "software requirements",
        "requirements engineering",
        "system design",
        "software architecture",
        "design pattern",
        "uml generation",
        "requirement analysis",
        "design synthesis"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I am interested in how LLMs assist early-stage software engineering tasks,
      including requirement understanding, system design, and architecture synthesis.
      I aim to explore how natural language and code reasoning can be combined
      to bridge requirement and design stages.

  # === â…¢. SE with LLMs: Software Development ===
  - name: "SoftwareDevelopment"
    label: "Software Development"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "code generation",
        "code completion",
        "code review",
        "documentation generation",
        "refactoring",
        "api recommendation",
        "pair programming",
        "IDE assistant"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I focus on LLM-based code generation and developer assistance.
      I want to understand how LLMs integrate into development environments,
      support refactoring, code review, and documentation, and improve programming productivity.

  # === â…£. SE with LLMs: Software Testing ===
  - name: "SoftwareTesting"
    label: "Software Testing"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "software testing",
        "test case generation",
        "assertion generation",
        "test repair",
        "test update",
        "test evolution",
        "mutation testing",
        "test coverage",
        "automated testing"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I am interested in test generation, repair, and evolution using LLMs.
      I want to study how static analysis, retrieval, and reasoning mechanisms
      can enhance the accuracy, coverage, and maintainability of automatically
      generated tests.

  # === â…¤. SE with LLMs: Software Maintenance ===
  - name: "SoftwareMaintenance"
    label: "Software Maintenance"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "program repair",
        "bug fixing",
        "defect localization",
        "code smell detection",
        "software evolution",
        "patch generation",
        "code migration"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I am interested in leveraging LLMs for software maintenance and evolution,
      including automatic bug fixing, refactoring, and patch generation.
      I focus on how LLMs can reason about code history, version differences,
      and semantic changes to improve repair quality.

  # === â…¥. SE with LLMs: Software Management ===
  - name: "SoftwareManagement"
    label: "Software Management"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "project management",
        "issue tracking",
        "developer productivity",
        "code review process",
        "software documentation",
        "traceability",
        "knowledge management"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I am interested in the use of LLMs for project and knowledge management,
      including issue triage, documentation generation, and developer support tools.
      I focus on enabling intelligent coordination and maintenance workflows.

  # === â…¦. Integration and Evaluation Dimensions ===
  - name: "IntegrationEvaluation"
    label: "Integration and Evaluation"
    query:
      categories: ["cs.SE", "cs.AI"]
      include: [
        "evaluation benchmark",
        "model tuning",
        "instruction tuning",
        "parameter efficient tuning",
        "model compression",
        "knowledge distillation",
        "empirical study",
        "education",
        "safety",
        "reliability",
        "hallucination mitigation"
      ]
      exclude: ["quantum", "biomedical"]
    interest_prompt: |
      I am interested in the evaluation and integration aspects of LLMs for software engineering.
      This includes benchmark design, fine-tuning, compression, safety, reliability,
      and empirical studies that assess model performance and trustworthiness.

      
relevance:
  # âœ… Dimension definitions for paper relevance, can add/remove/adjust weights based on interests
  scoring_dimensions:
    - name: "topic_alignment"       # Topic fit
      weight: 0.35
      description: "Whether paper research direction aligns with my focus areas"
    - name: "methodology_fit"       # Methodology match
      weight: 0.25
      description: "Whether proposed methods are directly relevant to my concerns"
    - name: "novelty"               # Innovation
      weight: 0.2
      description: "Whether paper provides novel contributions or perspectives"
    - name: "experiment_coverage"   # Experiment completeness
      weight: 0.2
      description: "Whether experiment/evaluation setup sufficiently answers key questions"

  # âœ… Pass threshold: total score >= this value considered "relevant", ultimately retained
  pass_threshold: 60

summarization:
  # âœ… Target number of TODO list items, actual generated count may vary slightly
  task_list_size: 5

  # âœ… Max key sections displayed in each summary, prevent overly long output
  max_sections: 4

site:
  # âœ… Static site output directory, recommend adding to .gitignore
  output_dir: "site"

  # âœ… External access base URL, update to actual address after GitHub Pages publishing
  base_url: "https://iseabot.github.io/LLM4ArxivPaper"

email:
  # âœ… Whether to enable email push (false = disabled)
  enabled: true

  # âœ… Sender email (must support SMTP), recommend using app-specific password
  sender: "${MAIL_USERNAME}"

  # âœ… Recipient list
  recipients: ["syeren@foxmail.com"]

  # âœ… SMTP server settings (recommend injecting sensitive info via environment variables)
  smtp_host: "smtp.gmail.com"
  smtp_port: 465
  username: "${MAIL_USERNAME}"
  password: "${MAIL_PASSWORD}"  # Recommend exporting environment variable before running
  use_tls: false
  use_ssl: true
  timeout: 30

  # âœ… Email subject template, can use {run_date}, {paper_count} placeholders
  subject_template: "ðŸ“‘ Weekly LLM4SE Paper Reading - {run_date}"

runtime:
  # âœ… Run mode:
  #    - online  : Call OpenAI API for real scoring & summarization (requires API Key)
  #    - offline : Use heuristic rules, suitable for local debugging or no-network scenarios
  mode: "online"

  # âœ… Limit paper count processed per topic; null means use fetch.max_papers_per_topic
  paper_limit: null
