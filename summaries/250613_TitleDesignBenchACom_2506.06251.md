**一句话概要**  
DesignBench 通过构建多框架、多任务的前端代码生成基准测试，系统评估了多模态大语言模型在自动化前端工程中的能力与局限。

**主体**  
当前多模态大语言模型（MLLMs）在自动化前端工程中展现出生成 UI 代码的潜力，但现有评测基准存在显著缺陷。主流前端开发框架（如 React、Vue）未被纳入评估，且测试仅聚焦代码生成而忽略实际开发中的编辑、修复等迭代环节，同时缺乏对任务难度、输入上下文等影响因素的深度分析。这些局限性导致现有基准无法真实反映 MLLMs 在复杂工程场景中的实用性。

为解决上述问题，作者提出 DesignBench 这一综合性评测体系。其核心创新在于覆盖三大主流框架（React/Vue/Angular）和原生 HTML/CSS，并模拟真实工作流中的三类关键任务：代码生成、编辑优化与缺陷修复。基准包含 900 个网页样本，涵盖 11 种主题、9 类编辑操作和 6 种错误类型，通过多维度设计（如框架适配性、任务复杂度、输入模态组合）构建细粒度评估矩阵。例如，通过控制输入图像的分辨率或添加自然语言注释，探究模型对多模态信息的融合能力。

实验结果表明，MLLMs 在不同框架下的表现存在显著差异，其中 React 代码生成准确率最高但 Angular 的组件化特性导致更多结构错误。任务维度上，模型在代码修复任务中的表现比生成任务低 23%，揭示出现有技术对复杂逻辑缺陷的处理瓶颈。研究还发现，输入图像中加入设计规范注释可使生成代码的功能完整性提升 18%，凸显多模态协同的重要性。这些发现为优化模型架构（如增强框架特定知识注入）和训练策略（如引入迭代修复机制）提供了明确方向。

**最后一句**  
该基准不仅为 MLLMs 在前端工程领域的性能优化树立了新标准，其多任务、多模态的评估范式也为其他代码生成场景的基准设计提供了可迁移的方法论启示。