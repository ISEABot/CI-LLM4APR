**一句话概要**  
作者提出基于大语言模型的智能体系统CodeContests+，通过生成高质量测试用例提升竞技编程数据集评估准确性，实验证明其显著优于原数据集并增强强化学习效果。

**主体**  
竞技编程因其高难度推理需求和精确反馈机制，已成为评估大语言模型推理能力的重要场景。然而现有公开数据集虽包含大量题目描述和参考答案，却普遍缺乏高质量的测试用例，这直接影响了模型评估的可靠性。研究团队发现，低质量测试用例会导致误判率上升，尤其在区分正确与错误代码时存在显著缺陷，成为制约模型训练与评估的关键瓶颈。

为解决这一问题，作者设计了一个基于大语言模型的智能体系统，通过多阶段协同工作流程生成具有强判别力的测试用例。该系统首先解析题目描述和参考代码，随后生成覆盖边界条件、极端输入和特殊场景的测试数据，并通过对抗性验证确保用例质量。研究将该系统应用于CodeContests数据集，构建出包含优化测试用例的新版本CodeContests+，其中测试用例的判别精度通过172万次带标签的代码提交进行了严格验证。

实验结果表明，优化后的测试用例将评估准确率显著提升，真阳性率（TPR）相比原始数据集实现突破性增长。在强化学习场景下的进一步测试显示，高质量测试用例使模型奖励信号更精确，最终模型性能提升幅度达23%。可视化分析揭示，新测试用例能有效捕捉代码中的逻辑漏洞，而传统用例则容易漏判语义错误。这项研究为构建可靠的程序评估体系提供了方法论支撑，其智能体框架可扩展至其他代码生成任务的测试验证场景。

**最后一句**  
该工作不仅解决了竞技编程评估的痛点，其测试用例生成范式对自动化软件测试和教育领域的智能编程辅助系统具有重要启示意义。