**一句话概要**  
研究探讨了大语言模型（LLMs）在竞赛级编程问题中生成可靠测试用例生成器的能力，揭示了其在生成针对性测试用例方面的局限性，并提出通过高质量数据集提升模型性能的方法。

**主体**  
尽管大语言模型在代码生成任务中表现出色，但其在代码调试和测试用例生成领域的潜力尚未充分挖掘。作者聚焦于竞赛级编程问题，提出TCGBench基准测试，旨在评估LLMs生成有效测试用例生成器的能力，尤其是能否生成针对人类编写代码中缺陷的测试用例。研究发现，当前最先进的LLMs虽然能够生成基本有效的测试用例生成器，但在生成针对性测试用例以暴露代码缺陷方面表现欠佳，甚至高级推理模型如o3-mini也显著落后于人类水平。

为解决这一问题，作者构建了一个高质量的人工标注数据集，包含生成针对性测试用例生成器的指令。实验表明，无论是通过提示工程还是微调，该数据集都能显著提升LLMs在生成针对性测试用例方面的性能。这一发现不仅验证了数据驱动方法在提升模型能力方面的有效性，也为未来研究提供了宝贵的资源。

**最后一句**  
该工作为LLMs在软件工程测试领域的应用开辟了新方向，同时强调了高质量数据在提升模型针对性任务表现中的关键作用。