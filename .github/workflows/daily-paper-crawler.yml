name: Daily Paper Crawler

on:
  # 每日定时运行 - 凌晨3点获取论文，上午8点发送邮件
  schedule:
    - cron: '0 19 * * *'  # UTC 19:00 = 北京时间 03:00 (论文爬取)
    - cron: '0 0 * * *'   # UTC 00:00 = 北京时间 08:00 (邮件报告)
  
  # 支持手动触发
  workflow_dispatch:
    inputs:
      start_date:
        description: '开始日期 (YYYY-MM-DD)'
        required: false
        default: ''
      end_date:
        description: '结束日期 (YYYY-MM-DD)'
        required: false
        default: ''
      email_only:
        description: '仅发送邮件报告'
        required: false
        default: 'false'
        type: boolean

jobs:
  crawl-papers:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create secrets.env
      run: |
        echo "LLM_API_KEY=${{ secrets.LLM_API_KEY }}" >> config/secrets.env
        echo "GH_TOKEN=${{ secrets.GH_TOKEN }}" >> config/secrets.env
        echo "EMAIL_PASSWORD=${{ secrets.EMAIL_PASSWORD }}" >> config/secrets.env

    - name: Clear proxy environment variables
      run: |
        # Clear any proxy environment variables that might interfere with OpenAI client
        unset HTTP_PROXY HTTPS_PROXY http_proxy https_proxy ALL_PROXY all_proxy || true
        echo "Cleared proxy environment variables"
        
    - name: Create logs directory
      run: mkdir -p logs
        
    - name: Determine run type
      id: run_type
      run: |
        echo "=== Debug Information ==="
        echo "Event name: ${{ github.event_name }}"
        echo "Email only input: '${{ github.event.inputs.email_only }}'"
        echo "Start date input: '${{ github.event.inputs.start_date }}'"
        echo "End date input: '${{ github.event.inputs.end_date }}'"
        echo "========================="

        # Check for date range first (highest priority for manual runs)
        if [ -n "${{ github.event.inputs.start_date }}" ] && [ -n "${{ github.event.inputs.end_date }}" ]; then
          echo "type=date_range" >> $GITHUB_OUTPUT
          echo "✅ Manual date range mode detected"
          echo "Date range: ${{ github.event.inputs.start_date }} to ${{ github.event.inputs.end_date }}"
        # Check for email-only mode
        elif [ "${{ github.event.inputs.email_only }}" = "true" ]; then
          echo "type=email" >> $GITHUB_OUTPUT
          echo "✅ Manual email-only mode detected"
        # Check for scheduled runs
        elif [ "${{ github.event_name }}" = "schedule" ]; then
          if [ "${{ github.event.schedule }}" = "0 0 * * *" ]; then
            echo "type=email" >> $GITHUB_OUTPUT
            echo "✅ Scheduled email report mode"
          else
            echo "type=crawl" >> $GITHUB_OUTPUT
            echo "✅ Scheduled crawl mode"
          fi
        # Default to crawl mode
        else
          echo "type=crawl" >> $GITHUB_OUTPUT
          echo "✅ Default crawl mode (manual trigger without specific parameters)"
        fi
        
    - name: Run paper crawler
      run: |
        echo "=== Executing Paper Crawler ==="
        echo "Selected mode: ${{ steps.run_type.outputs.type }}"
        echo "==============================="

        case "${{ steps.run_type.outputs.type }}" in
          "email")
            echo "🔄 Running email-only mode..."
            echo "This will send email report based on existing summaries"
            python src/main.py --email-only
            ;;
          "date_range")
            echo "🔄 Running date range mode..."
            echo "Date range: ${{ github.event.inputs.start_date }} to ${{ github.event.inputs.end_date }}"
            echo "This will: 1) Crawl arXiv papers, 2) Generate LLM summaries, 3) Upload to GitHub"
            python src/main.py --date-range --start-date "${{ github.event.inputs.start_date }}" --end-date "${{ github.event.inputs.end_date }}"
            ;;
          "crawl")
            echo "🔄 Running daily crawl mode..."
            echo "This will: 1) Crawl arXiv papers, 2) Generate LLM summaries, 3) Upload to GitHub"
            python src/main.py --daily --days-back 1
            ;;
          *)
            echo "❌ Unknown mode: ${{ steps.run_type.outputs.type }}"
            echo "Available modes: email, date_range, crawl"
            exit 1
            ;;
        esac
        
    - name: Upload logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: logs-${{ github.run_number }}
        path: logs/
        retention-days: 7
        
    - name: Check for failures
      if: failure()
      run: |
        echo "Job failed. Check logs for details."
        if [ -f logs/llm4reading.log ]; then
          echo "Last 50 lines of log:"
          tail -50 logs/llm4reading.log
        fi

  # 可选：发送通知到Slack/Discord等
  notify:
    needs: crawl-papers
    runs-on: ubuntu-latest
    if: always()
    
    steps:
    - name: Notify success
      if: needs.crawl-papers.result == 'success'
      run: |
        echo "✅ Daily paper crawler completed successfully"
        
    - name: Notify failure
      if: needs.crawl-papers.result == 'failure'
      run: |
        echo "❌ Daily paper crawler failed"
        echo "Check the logs for more details"
